{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7603590-e1cd-4006-bc7b-73718558dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -q transformers==4.40.2 torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c54654-266a-482b-b13b-4de85f9f0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MAX_INPUT_SIZE = 20_000\n",
    "MAX_NEW_TOKENS = 6000\n",
    "\n",
    "model_name = \"numind/NuExtract-v1.5\"\n",
    "device = \"cuda\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n",
    "def clean_json_text(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"\\#\", \"#\").replace(\"\\&\", \"&\")\n",
    "    return text\n",
    "\n",
    "def predict_chunk(text, template, current, model, tokenizer):\n",
    "    current = clean_json_text(current)\n",
    "\n",
    "    input_llm =  f\"<|input|>\\n### Template:\\n{template}\\n### Current:\\n{current}\\n### Text:\\n{text}\\n\\n<|output|>\" + \"{\"\n",
    "    input_ids = tokenizer(input_llm, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_SIZE).to(\"cuda\")\n",
    "    output = tokenizer.decode(model.generate(**input_ids, max_new_tokens=MAX_NEW_TOKENS)[0], skip_special_tokens=True)\n",
    "\n",
    "    return clean_json_text(output.split(\"<|output|>\")[1])\n",
    "\n",
    "def split_document(document, window_size, overlap):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    print(f\"\\tLength of document: {len(tokens)} tokens\")\n",
    "\n",
    "    chunks = []\n",
    "    if len(tokens) > window_size:\n",
    "        for i in range(0, len(tokens), window_size-overlap):\n",
    "            print(f\"\\t{i} to {i + len(tokens[i:i + window_size])}\")\n",
    "            chunk = tokenizer.convert_tokens_to_string(tokens[i:i + window_size])\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            if i + len(tokens[i:i + window_size]) >= len(tokens):\n",
    "                break\n",
    "    else:\n",
    "        chunks.append(document)\n",
    "    print(f\"\\tSplit into {len(chunks)} chunks\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def handle_broken_output(pred, prev):\n",
    "    try:\n",
    "        if all([(v in [\"\", []]) for v in json.loads(pred).values()]):\n",
    "            # if empty json, return previous\n",
    "            pred = prev\n",
    "    except:\n",
    "        # if broken json, return previous\n",
    "        pred = prev\n",
    "\n",
    "    return pred\n",
    "\n",
    "def sliding_window_prediction(text, template, model, tokenizer, window_size=4000, overlap=128):\n",
    "    # split text into chunks of n tokens\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = split_document(text, window_size, overlap)\n",
    "\n",
    "    # iterate over text chunks\n",
    "    prev = template\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i}...\")\n",
    "        pred = predict_chunk(chunk, template, prev, model, tokenizer)\n",
    "\n",
    "        # handle broken output\n",
    "        pred = handle_broken_output(pred, prev)\n",
    "            \n",
    "        # iterate\n",
    "        prev = pred\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ebe021-fb1e-463a-9a7f-9d9d9f55f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Le contingent annuel d’heures supplémentaires est fixé à 220 heures. Le premier taux de majoration est de 25% et s’applique de la 36e à la 43e heure.\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"{\n",
    "    \"base_legale_hebdomadaire\": \"\",\n",
    "    \"duree_annuel_heures\": \"\" ,\n",
    "    \"contingent_annuel_heures_supplementaires\": \"\",\n",
    "    \"premier_taux_majoration\": \"\",\n",
    "    \"plage_premier_taux_majoration\": \"\",\n",
    "    \"deuxieme_taux_majoration\": \"\",\n",
    "    \"plage_deuxieme_taux_majoration\": \"\",\n",
    "    \"troisieme_taux_majoration\": \"\",\n",
    "    \"plage_troisieme_taux_majoration\": \"\",\n",
    "    \"presence_repos_compensateur_remplacement\": \"\",\n",
    "    \"taux_majoration_contrepartie_obligatoire_en_repos\": \"\",\n",
    "    \"delai_prevenance\": \"\"\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f64b3-f1cc-4774-956b-8361a0b8d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_prediction(text, template, model, tokenizer, window_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "my-uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
